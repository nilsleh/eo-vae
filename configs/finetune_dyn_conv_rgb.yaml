
experiment:
  experiment_name: "eo-vae-finetune"
  exp_dir: "results/exps/eo-vae/finetune"

wandb:
  project: eo-vae
  entity: nleh
  mode: online

model:
  _target_: eo_vae.models.autoencoder_flux.FluxAutoencoderKL
  
  freeze_body: True

  base_lr: 2e-4
  final_lr_sched: 2e-5
  warmup_epochs: 1
  decay_end_epoch: ${trainer.max_epochs}
  clip_grad: 1.0

  # Loss configuration
  loss_fn:
    _target_: eo_vae.models.modules.loss_functions.EOGenerativeLoss
    perceptual_weight: 1.0
    disc_weight: 0.7
    gan_start_step: ${eval:'4 * 2000'}  # 4 epochs * ~1000 steps
    disc_update_start_step: ${eval:'2* 2000'}  # 2 epochs
    # lpips_start_step: 0
    max_d_weight: 10000.0
    # disc_updates_per_step: 1
    disc_loss_type: "hinge"
    discriminator:
      _target_: eo_vae.models.modules.loss_utils.DOFADiscriminator
      dofa_net:
        _target_: eo_vae.models.dofa.dofav2_base_patch14_224
        model_bands: ["RED", "GREEN", "BLUE"]
        ckpt_data: /mnt/SSD2/nils/eo-rae/eo_rae/models/dofav2/dofav2_vit_base_e150.pth
    lpips:
      _target_: eo_vae.models.modules.loss_utils.DOFALPIPS
      dofa_net:
        _target_: eo_vae.models.dofa.dofav2_base_patch14_224
        model_bands: ["RED", "GREEN", "BLUE"]
        ckpt_data: /mnt/SSD2/nils/eo-rae/eo_rae/models/dofav2/dofav2_vit_base_e150.pth
  
  encoder:
    _target_: eo_vae.models.Encoder
    z_channels: 32
    resolution: 256
    in_channels: 3
    ch: 128
    ch_mult: [ 1,2,4,4 ]  # num_down = len(ch_mult)-1
    num_res_blocks: 2
    use_dynamic_ops: True

  decoder:
    _target_: eo_vae.models.Decoder
    z_channels: 32
    resolution: 256
    out_ch: 3
    ch: 128
    ch_mult: [ 1,2,4,4 ]  # num_down = len(ch_mult)-1
    num_res_blocks: 2
    use_dynamic_ops: True


datamodule:
  _target_: eo_vae.datasets.terramesh_datamodule.TerraMeshDataModule
  data_path: "/mnt/SSD2/nils/datasets/terramesh"
  modalities: ["S2RGB"]  # Load both modalities
  batch_size: 16
  eval_batch_size: 32
  num_workers: 4
  train_collate_mode: "random"  # Randomly select modality per batch
  val_collate_mode: "S2RGB"     # Always use S2L2A for validation
  normalize: true
  target_size: [224, 224]

trainer:
  _target_: lightning.Trainer
  max_epochs: 50
  accelerator: gpu
  #strategy: ddp
  # strategy: ddp_find_unused_parameters_true
  limit_val_batches: 100
  limit_train_batches: 2000 # choose 1000 iterable dataset batches to have control over params
  devices: [3]
  #devices: [0,1,2,3]
  # accumulate_grad_batches: 2
  #gradient_clip_val: 1.0
  log_every_n_steps: 100
  #overfit_batches: 10
  

flux_chkpt: /mnt/SSD2/nils/eo-vae/checkpoints/ae.safetensors